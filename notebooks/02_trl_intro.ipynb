{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRL Introduction: RL on Actual LLMs\n",
    "\n",
    "Now we apply the concepts from notebook 01 to real language models.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Load a small LLM (Qwen-2.5-0.5B)\n",
    "2. Create a simple reward function\n",
    "3. Run GRPO training using HuggingFace's `trl` library\n",
    "\n",
    "**Why GRPO over PPO?**\n",
    "- GRPO doesn't need a separate value model (simpler)\n",
    "- Uses group mean as baseline (like we learned in notebook 01)\n",
    "- This is what DeepSeek used for their reasoning models\n",
    "\n",
    "**This notebook is for understanding.** The actual training runs on Modal via:\n",
    "```bash\n",
    "modal run modal_app.py::train_grpo_simple\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Components\n",
    "\n",
    "Let's understand what goes into GRPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 1: MODEL\n",
    "# The LLM we're training (the \"policy\")\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Small, fast for learning\n",
    "\n",
    "# Component 2: DATASET\n",
    "# Prompts to train on - simple arithmetic to verify our setup works\n",
    "TRAIN_DATA = [\n",
    "    {\"prompt\": \"What is 2 + 3?\", \"answer\": \"5\"},\n",
    "    {\"prompt\": \"What is 7 - 4?\", \"answer\": \"3\"},\n",
    "    {\"prompt\": \"What is 5 * 2?\", \"answer\": \"10\"},\n",
    "    {\"prompt\": \"What is 8 / 2?\", \"answer\": \"4\"},\n",
    "    {\"prompt\": \"What is 10 + 5?\", \"answer\": \"15\"},\n",
    "    {\"prompt\": \"What is 9 - 3?\", \"answer\": \"6\"},\n",
    "    {\"prompt\": \"What is 4 * 3?\", \"answer\": \"12\"},\n",
    "    {\"prompt\": \"What is 12 / 4?\", \"answer\": \"3\"},\n",
    "]\n",
    "\n",
    "# Component 3: REWARD FUNCTION\n",
    "# Score the model's output - this is the \"verifiable\" in RLVR\n",
    "def compute_reward(response: str, correct_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple reward: 1.0 if correct answer appears in response, 0.0 otherwise.\n",
    "    \"\"\"\n",
    "    if correct_answer in response:\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "\n",
    "# Test reward function\n",
    "print(\"Reward tests:\")\n",
    "print(f\"  'The answer is 5' for '5': {compute_reward('The answer is 5', '5')}\")\n",
    "print(f\"  'I think it is 7' for '5': {compute_reward('I think it is 7', '5')}\")\n",
    "print(f\"  '5' for '5': {compute_reward('5', '5')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: How GRPO Works\n",
    "\n",
    "`trl.GRPOTrainer` does this:\n",
    "\n",
    "```\n",
    "For each batch of prompts:\n",
    "    1. Generate K responses per prompt (the \"group\", e.g., K=4)\n",
    "    2. Compute reward for each response\n",
    "    3. Compute baseline = mean(rewards in group)\n",
    "    4. For each response:\n",
    "         advantage = reward - baseline\n",
    "         loss += -advantage * log_prob(response)\n",
    "    5. Update model\n",
    "```\n",
    "\n",
    "This is exactly what we did in notebook 01 with the bandit, but now:\n",
    "- \"action\" = entire generated sequence\n",
    "- \"policy\" = the LLM\n",
    "- \"baseline\" = group mean (GRPO's approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training hyperparameters\n",
    "\n",
    "GRPO_CONFIG = {\n",
    "    # How many responses to generate per prompt (the \"group\")\n",
    "    \"num_generations\": 4,\n",
    "    \n",
    "    # Learning rate - small for LLMs!\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \n",
    "    # Batch size - must be divisible by num_generations\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \n",
    "    # How many update steps to run\n",
    "    \"max_steps\": 50,\n",
    "    \n",
    "    # Max length of generated responses\n",
    "    \"max_completion_length\": 64,\n",
    "    \n",
    "    # Temperature for sampling (higher = more random = more exploration)\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "print(\"GRPO Config:\")\n",
    "for k, v in GRPO_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Training Code\n",
    "\n",
    "Here's what `modal_app.py::train_grpo_simple` does (simplified):\n",
    "\n",
    "```python\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Define reward function\n",
    "def reward_fn(completions, prompts, **kwargs):\n",
    "    rewards = []\n",
    "    for completion, prompt in zip(completions, prompts):\n",
    "        correct = answer_lookup[prompt]\n",
    "        rewards.append(1.0 if correct in completion else 0.0)\n",
    "    return rewards\n",
    "\n",
    "# Create trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    config=GRPOConfig(...),\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    reward_funcs=reward_fn,  # This is where RLVR happens!\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "That's it. The `GRPOTrainer` handles all the RL mechanics we learned in notebook 01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Run It\n",
    "\n",
    "Run on Modal (needs GPU):\n",
    "\n",
    "```bash\n",
    "modal run modal_app.py::train_grpo_simple\n",
    "```\n",
    "\n",
    "**What to watch for:**\n",
    "1. **Reward increasing** - Model learns to produce correct answers\n",
    "2. **Loss** - Will fluctuate (stochastic!), but trend should improve\n",
    "3. **Test outputs** - At the end, it tests on 3 examples\n",
    "\n",
    "**Expected:**\n",
    "- Takes ~5-10 minutes\n",
    "- Costs ~$0.10-0.20\n",
    "- Model already knows arithmetic, so we're mostly teaching output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running this:\n",
    "1. **03_gsm8k_reward.ipynb** - Build proper answer extraction for GSM8K (real math problems)\n",
    "2. **04_pseudo_labeling.ipynb** - Implement consensus-based pseudo-labeling\n",
    "3. **Full noisy student loop** - Combine everything"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
